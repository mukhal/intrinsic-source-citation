## experiment
experiment:
  name: sequential_bioS_single_doc_url_cot_prefix_augment_NO_AUGMENT
  output_dir: outputs/experiments/paper
  cache_dir: /net/nfs.cirrascale/allennlp/muhammadk/cache

## data
data:
  text_data_path: ../data/bio-attribution-bioS/100K_mbpd_4_aug_3_last_name_prefix/text/
  augment:
    doc:
      do: false ## shouldn't shuffle for CoT
      method: permute
      n_sample_per_doc: 2
  finetune:
    number_non_attributable_negatives: 0
    neg_create_probability: 0.0


## model
model:
  name: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T

train:
  url_location: last
  pretrain: false
  sequential: true
  #### URL representation learning task ####
  finetune_q_url_a: false
  repeat_url_across_doc: false
  #### Attribution task ####
  finetune_q_a_url: false
  finetune_q_a: false
  finetune_q_a_doc_url: true
  q_a_url_predict_url_only: false
  ood_attribution_percentage: 0.0
  ## loss and attn config 
  cross_doc_attention: false
  url_loss_factor: 1.0
  loss_type: mask
  config_template_path: conf/full-pipeline/train_config_template_llama.yaml
  device_eval_batch_size: 40
  device_train_microbatch_size: 4
  eval_first: true
  weight_decay: 0.01
  lr: 8.0e-5
  max_duration: 10ep

eval:  
  disable_qa_eval: false
  disable_all_eval: false
  disable_attribution_eval: false
  disable_non_attrib_eval: true
  icl_eval: true
  ppl_eval: true

