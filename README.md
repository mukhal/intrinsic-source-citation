# Source-Aware Training Enables Knowledge Attribution in Language Models

![image](https://github.com/mukhal/intrinsic-source-citation/assets/5109053/9f4d582e-5b92-4715-88ab-97d20f82ee04)

## Data
You can access our synthetic pretraining dataset (BioCite) on ðŸ¤—: https://huggingface.co/datasets/mkhalifa/BioCite/tree/main/qa

**BioCite stats:** 
|                          | Size    |
|--------------------------|---------|
| **Pretraining**          |         |
| \#documents              | 100K    |
| \#facts/sents            | 408K    |
| \#tokens                 | 5.7M    |
| avg. sents per doc       | 4.1     |
| avg. tokens per doc      | 56.9    |
| **Instruction tuning**   |         |
| \#examples               | 186K    |
| \#tokens                 | 3.1M    |

